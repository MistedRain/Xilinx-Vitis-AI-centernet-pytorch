/*
 * Copyright 2019 Xilinx Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#include <dirent.h>
#include <sys/stat.h>
#include <zconf.h>

#include <algorithm>
#include <atomic>
#include <chrono>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <mutex>
#include <opencv2/opencv.hpp>
#include <queue>
#include <string>
#include <thread>
#include <vector>
#include <ctime>
#include <stdlib.h>
#include "common.h"
#include "utils.h"

using namespace std;
using namespace cv;
using namespace std::chrono;

int idxInputImage = 0;  // frame index of input video
int idxShowImage = 0;   // next frame index to be displayed
bool bReading = true;   // flag of reding input frame
bool bExiting = false;

int im_w = 0;
int im_h = 0;
int new_w = 0;
int new_h = 0;
chrono::system_clock::time_point start_time;

typedef pair<int, Mat> imagePair;
class paircomp {
 public:
  bool operator()(const imagePair& n1, const imagePair& n2) const {
    if (n1.first == n2.first) {
      return (n1.first > n2.first);
    }

    return n1.first > n2.first;
  }
};

// mutex for protection of input frames queue
mutex mtxQueueInput;
// mutex for protecFtion of display frmaes queue
mutex mtxQueueShow;
// input frames queue
queue<pair<int, int8_t *>> queueInput_int8;
queue<pair<int, Mat>> queueInput;
// display frames queue
priority_queue<imagePair, vector<imagePair>, paircomp> queueShow;

// std::vector<const xir::Tensor*>inputTensors1,outputTensors1;
// std::vector<const xir::Tensor*>inputTensors2,outputTensors2;
float input_scale;
vector<float> output_scale;

// std::vector<std::unique_ptr<vart::TensorBuffer>> inputs, outputs;
// std::vector<vart::TensorBuffer*> inputsPtr, outputsPtr;
// std::vector<std::shared_ptr<xir::Tensor>> batchTensors;

const string classes[20] = {"aeroplane",
"bicycle",
"bird",
"boat",
"bottle",
"bus",
"car",
"cat",
"chair",
"cow",
"diningtable",
"dog",
"horse",
"motorbike",
"person",
"pottedplant",
"sheep",
"sofa",
"train",
"tvmonitor"};

GraphInfo shapes;

static float get_input_scale(const xir::Tensor* tensor) {
  int fixpos = tensor->template get_attr<int>("fix_point");
  return std::exp2f(1.0f * (float)fixpos);
}
// fix_point to scale for output tensor
static float get_output_scale(const xir::Tensor* tensor) {
  int fixpos = tensor->template get_attr<int>("fix_point");
  return std::exp2f(-1.0f * (float)fixpos);
}




void readFrame(VideoCapture video){
    int width = shapes.inTensorList[0].width;
    int height = shapes.inTensorList[0].height;
    int size = shapes.inTensorList[0].size;

    

    
    if (((float)width / im_w) < ((float)height / im_h)) {
        new_w = width;
        new_h = (im_h * width) / im_w;
    } else {
        new_h = height;
        new_w = (im_w * height) / im_h;
    }

    start_time = chrono::system_clock::now();

    while (true) {
        Mat img;
        if (queueInput.size() < 30) {
            if (!video.read(img)) {
                break;
            }
            

            
            mtxQueueInput.lock();
            queueInput.push(make_pair(idxInputImage++, img));
            // queueInput_int8.push(make_pair(idxInputImage++, data));
            mtxQueueInput.unlock();
        } else {
            usleep(10);
        }
    }

    video.release();
    bExiting = true;
}
void topK(vector<vector<int>> &TOPk,vector<float>result,int k,int height,int width,int channel){

    priority_queue< pair<float , vector<int>> >q;
    for (int c = 0; c < channel; ++c) {
        for (int h = 0; h < height; ++h) {
            for (int w = 0; w < width; ++w) {
                int temp = c * height * width + h * width + w;
                
                if(result[temp] > 0.5){
                    vector<int>tmp;
                    tmp.push_back(c);
                    tmp.push_back(h);
                    tmp.push_back(w);
                    q.push(pair<float, vector<int>>(result[temp],tmp));
                }
               
            }
        }
    }

    k = k > q.size() ? q.size():k;
    for (auto i = 0; i < k; ++i) {
        pair<float , vector<int>> ki = q.top();
        TOPk.push_back(ki.second);
        q.pop();
  }
}

void postProcess(vart::Runner* runner, Mat& frame, vector<int8_t*> &results,
                 int sWidth, int sHeight, const float* output_scale) {
  

    vector<vector<float>> results_float;
    vector<vector<int>> TopK;
    int width = shapes.outTensorList[0].width;
    int height = shapes.outTensorList[0].height;
  for (int ii = 0; ii < 3; ii++) {
    // int width = shapes.outTensorList[ii].width;
    // int height = shapes.outTensorList[ii].height;
    int channel = shapes.outTensorList[ii].channel;
    int sizeOut = channel * width * height;
    vector<float>result(sizeOut);
    

    

    /* Store every output node results */
    get_output(results[ii], sizeOut, channel, height, width, output_scale[ii],result);
    // vector<float>swapA(result);
   
    if(ii == 0){
        for (int c = 0; c < channel; ++c) {
            for (int h = 0; h < height; ++h) {
                for (int w = 0; w < width; ++w) {    
                    int temp = c * height * width + h * width + w;
                    
                    result[temp] = sigmoid(result[temp]);
                }
            }
        }
        vector<float>result_tmp(sizeOut);
        forward_maxpool_layer(width,height,channel,height,width,1,3,1,result.data(),result_tmp.data());
        // result.assign(result_tmp.begin(),result_tmp.end());
        result = result_tmp;
        topK(TopK,result,100,height,width,channel);
    }
    results_float.push_back(result);
  }

    int frame_width = frame.cols;
    int frame_height = frame.rows;
    
    float frame_scale_w;
    float frame_scale_h;
    if(frame_width>=frame_height){
        frame_scale_w = 1;
        frame_scale_h = frame_width / float(frame_height);
    }
    else{
        frame_scale_h = 1;
        frame_scale_w = frame_height / float(frame_width);
    }

    for(auto &tmp : TopK){
        int c = tmp[0];
        int h = tmp[1];
        int w = tmp[2];
        int temp = c * height * width + h * width + w;
        //cout<<classes[c]<<" "<<results_float[0][temp]<<endl;
        if(results_float[0][temp] > CONF){
            float out_w = results_float[1][h * width + w] *1.0 / float(width);
            float out_h = results_float[1][height * width + h * width + w] *1.0 / float(height);
            float x = (results_float[2][h * width + w] + w)/ float(width) ;
            float y = (results_float[2][height * width + h * width + w] + h)/ float(height) ;

            float x_min = (x - out_w/2.0)*(frame_width)*frame_scale_w - (frame_width*frame_scale_w - frame_width)/2+ 1;
            float x_max = (x + out_w/2.0)*(frame_width)*frame_scale_w - (frame_width*frame_scale_w - frame_width)/2 + 1;

            float y_min = (y - out_h/2.0)*(frame_height)*frame_scale_h - (frame_height*frame_scale_h - frame_height)/2+ 1;
            float y_max = (y + out_h/2.0)*(frame_height)*frame_scale_h - (frame_height*frame_scale_h - frame_height)/2 + 1;

            rectangle(frame, Point(x_min, y_min), Point(x_max, y_max),Scalar(0, 255, 255), 1, 1, 0);
        }
    }

}
void runCenterNet(vart::Runner* runner){

    auto inputTensors = runner->get_input_tensors();
    auto outputTensors = runner->get_output_tensors();

    auto out_dims0 = outputTensors[0]->get_shape();
    auto out_dims1 = outputTensors[1]->get_shape();
    auto out_dims2 = outputTensors[2]->get_shape();
    auto in_dims = inputTensors[0]->get_shape();

    int width = shapes.inTensorList[0].width;
    int height = shapes.inTensorList[0].height;
    int size = shapes.inTensorList[0].size;

    int8_t* data = new int8_t[shapes.inTensorList[0].size * inputTensors[0]->get_shape().at(0)];
    int8_t* result0 = new int8_t[shapes.outTensorList[0].size * outputTensors[shapes.output_mapping[0]]->get_shape().at(0)];
    int8_t* result1 = new int8_t[shapes.outTensorList[1].size * outputTensors[shapes.output_mapping[1]]->get_shape().at(0)];
    int8_t* result2 = new int8_t[shapes.outTensorList[2].size * outputTensors[shapes.output_mapping[2]]->get_shape().at(0)];

    vector<int8_t*> result;
    result.push_back(result0);
    result.push_back(result1);
    result.push_back(result2);

    std::vector<std::unique_ptr<vart::TensorBuffer>> inputs, outputs;
    std::vector<vart::TensorBuffer*> inputsPtr, outputsPtr;
    std::vector<std::shared_ptr<xir::Tensor>> batchTensors;
    while (true)
    {
        pair<int, Mat> pairIndexImage;
        // pair<int,int8_t*>pairIndexInputData;

        mtxQueueInput.lock();
        if (queueInput.empty()) {
            mtxQueueInput.unlock();
            if (bExiting) break;
            if (bReading) {
                continue;
            }
            else {
                break;
            }
        } 
        else {
        /* get an input frame from input frames queue */
            pairIndexImage = queueInput.front();
            // pairIndexInputData = queueInput_int8.front();
            // queueInput_int8.pop();
            queueInput.pop();
            mtxQueueInput.unlock();
        }

    vector<float>bb(size);
    letterbox_image(pairIndexImage.second,bb,width,height,new_w,new_h);
    float scale = pow(2, 7);
    for (int i = 0; i < size; ++i) {
        data[i] = (int8_t)(bb.data()[i] * input_scale);
        if (data[i] < 0) data[i] = (int8_t)((float)(127 / scale) * input_scale);
    }
    //input
        batchTensors.push_back(std::shared_ptr<xir::Tensor>(
            xir::Tensor::create(inputTensors[0]->get_name(), in_dims,
                                xir::DataType{xir::DataType::XINT, 8u})));
        inputs.push_back(std::make_unique<CpuFlatTensorBuffer>(
            data, batchTensors.back().get()));
    //output
        batchTensors.push_back(std::shared_ptr<xir::Tensor>(
            xir::Tensor::create(outputTensors[0]->get_name(), out_dims0,
                                xir::DataType{xir::DataType::XINT, 8u})));
        outputs.push_back(std::make_unique<CpuFlatTensorBuffer>(
            result[0], batchTensors.back().get()));
        batchTensors.push_back(std::shared_ptr<xir::Tensor>(
            xir::Tensor::create(outputTensors[1]->get_name(), out_dims1,
                                xir::DataType{xir::DataType::XINT, 8u})));
        outputs.push_back(std::make_unique<CpuFlatTensorBuffer>(
            result[1], batchTensors.back().get()));
        batchTensors.push_back(std::shared_ptr<xir::Tensor>(
            xir::Tensor::create(outputTensors[2]->get_name(), out_dims2,
                                xir::DataType{xir::DataType::XINT, 8u})));
        outputs.push_back(std::make_unique<CpuFlatTensorBuffer>(
            result[2], batchTensors.back().get()));
        
        inputsPtr.push_back(inputs[0].get());
        outputsPtr.resize(3);
        outputsPtr[shapes.output_mapping[0]] = outputs[0].get();
        outputsPtr[shapes.output_mapping[1]] = outputs[1].get();
        outputsPtr[shapes.output_mapping[2]] = outputs[2].get();

        auto job_id = runner->execute_async(inputsPtr, outputsPtr);
        runner->wait(job_id.first, -1);

        postProcess(runner, pairIndexImage.second, result, width, height,output_scale.data());

        mtxQueueShow.lock();

    /* push the image into display frame queue */
        queueShow.push(pairIndexImage);
        mtxQueueShow.unlock();
        batchTensors.clear();
        inputs.clear();
        outputs.clear();
        inputsPtr.clear();
        outputsPtr.clear();
    }
    delete[] result0;
    delete[] result1;
    delete[] result2;
    delete[] data;

}
void displayFrame(){
    Mat frame;

    while (true) {
    if (bExiting) break;
    mtxQueueShow.lock();

    if (queueShow.empty()) {
        mtxQueueShow.unlock();
        usleep(10);
    } else if (idxShowImage == queueShow.top().first) {
        auto show_time = chrono::system_clock::now();
        stringstream buffer;
        frame = queueShow.top().second;
        if (frame.rows <= 0 || frame.cols <= 0) {
            mtxQueueShow.unlock();
            continue;
        }
        auto dura = (duration_cast<microseconds>(show_time - start_time)).count();
        buffer << fixed << setprecision(1)
                << (float)queueShow.top().first / (dura / 1000000.f);
        string a = buffer.str() + " FPS";
        cv::putText(frame, a, cv::Point(10, 15), 1, 1, cv::Scalar{0, 0, 240}, 1);
        cv::imshow("CenterNet Detection@Xilinx DPU", frame);

        idxShowImage++;
        queueShow.pop();
        mtxQueueShow.unlock();
        if (waitKey(1) == 'q') {
            bReading = false;
            exit(0);
        }
    } 
    else {
        mtxQueueShow.unlock();
        }
    }
}
int main(const int argc, const char** argv) {
    if (argc != 3) {
    cout << "Usage of CenterNet detection: " << argv[0]
            << "<device_num> <model_file>" << endl;
    return -1;
    }


    auto graph = xir::Graph::deserialize(argv[2]);
    auto subgraph = get_dpu_subgraph(graph.get());
    CHECK_EQ(subgraph.size(), 1u)
        << "CenterNet should have one and only one dpu subgraph.";
    LOG(INFO) << "create running for subgraph: " << subgraph[0]->get_name();

    auto runner1 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner2 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner3 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner4 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner5 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner6 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner7 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner8 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner9 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner10 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner11 = vart::Runner::create_runner(subgraph[0], "run");
    auto runner12 = vart::Runner::create_runner(subgraph[0], "run");
    // get in/out tenosrs
    auto inputTensors = runner1->get_input_tensors();
    auto outputTensors = runner1->get_output_tensors();
    int inputCnt = inputTensors.size();
    int outputCnt = outputTensors.size();
    // init the shape info
    TensorShape inshapes[inputCnt];
    TensorShape outshapes[outputCnt];
    shapes.inTensorList = inshapes;
    shapes.outTensorList = outshapes;
    getTensorShape(runner1.get(), &shapes, inputCnt,outputCnt);

    input_scale = get_input_scale(runner1->get_input_tensors()[0]);
    for (int i; i < 3; i++) {
        output_scale.push_back(get_output_scale(runner1->get_output_tensors()[shapes.output_mapping[i]]));
    }

    
    
    string filename = argv[1];
    VideoCapture capture;
   
    if(filename != "0" ){
        capture.open(filename);
    }
    else{
        capture.open(atoi(argv[1]));
    }
    
    if(!capture.isOpened()){
        cout<<"open cam failed !"<<endl;
        return -1;
    }
    Mat tmpMat;
    if (!capture.read(tmpMat)) {
                return -1;
    }

    im_w = tmpMat.cols;
    im_h = tmpMat.rows;

    array<thread,14> threadsList = {
      thread(readFrame, capture), thread(runCenterNet,runner1.get()),thread(runCenterNet,runner2.get()),
      thread(runCenterNet,runner3.get()),thread(runCenterNet,runner4.get()),thread(runCenterNet,runner5.get()),
      thread(runCenterNet,runner6.get()),thread(runCenterNet,runner7.get()),thread(runCenterNet,runner8.get()),
      thread(runCenterNet,runner9.get()),thread(runCenterNet,runner10.get()),thread(runCenterNet,runner11.get()),
      thread(runCenterNet,runner12.get()),
      thread(displayFrame)
      };
  
    for (int i = 0; i < 14; i++) {
        threadsList[i].join();
    }
    
    


  return 0;
}
